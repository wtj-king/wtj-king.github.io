<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2020/06/23/zong-jie/"/>
      <url>/2020/06/23/zong-jie/</url>
      
        <content type="html"><![CDATA[<h3 id="爬虫的概念"><a href="#爬虫的概念" class="headerlink" title="爬虫的概念"></a>爬虫的概念</h3><ul><li>爬虫是模拟浏览器发送请求，获取响应</li></ul><h3 id="爬虫的流程"><a href="#爬虫的流程" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><ul><li>url—&gt;发送请求，获取响应—&gt;提取数据—》保存</li><li>发送请求，获取响应—&gt;提取url</li></ul><h4 id="爬虫要根据当前url地址对应的响应为准-，当前url地址的elements的内容和url的响应不一样"><a href="#爬虫要根据当前url地址对应的响应为准-，当前url地址的elements的内容和url的响应不一样" class="headerlink" title="爬虫要根据当前url地址对应的响应为准 ，当前url地址的elements的内容和url的响应不一样"></a>爬虫要根据当前url地址对应的响应为准 ，当前url地址的elements的内容和url的响应不一样</h4><h3 id="页面上的数据在哪里"><a href="#页面上的数据在哪里" class="headerlink" title="页面上的数据在哪里"></a>页面上的数据在哪里</h3><ul><li>当前url地址对应的响应中</li><li>其他的url地址对应的响应中<ul><li>比如ajax请求中</li></ul></li><li>js生成的<ul><li>部分数据在响应中</li><li>全部通过js生成</li></ul></li></ul><h3 id="requests中解决编解码的方法"><a href="#requests中解决编解码的方法" class="headerlink" title="requests中解决编解码的方法"></a>requests中解决编解码的方法</h3><ul><li>response.content.decode()</li><li>response.content.decode(“gbk”)</li><li>response.text<h3 id="爬虫的概念-1"><a href="#爬虫的概念-1" class="headerlink" title="爬虫的概念"></a>爬虫的概念</h3><ul><li>模拟浏览器发送请求，获取响应</li></ul></li></ul><h3 id="爬虫的流程-1"><a href="#爬虫的流程-1" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><ul><li>url—》发送请求，获取响应—》提取数据—》保存</li><li>发送请求，获取响应—》提取url（下一页，详情页）重新请求</li></ul><h3 id="爬虫要根据当前url地址对应的响应为准"><a href="#爬虫要根据当前url地址对应的响应为准" class="headerlink" title="爬虫要根据当前url地址对应的响应为准"></a>爬虫要根据当前url地址对应的响应为准</h3><ul><li>爬虫只会请求当前这个url，但是不是请求js，</li><li>浏览器拿到的内容，我们在浏览器中看到的内容是elements里面的内容</li><li>elements=url对应的响应+js+css+图片</li></ul><h3 id="requests模块如何发送请求"><a href="#requests模块如何发送请求" class="headerlink" title="requests模块如何发送请求"></a>requests模块如何发送请求</h3><ul><li>resposne = requests.get(url)</li></ul><h3 id="requests中解决编解码的方法-1"><a href="#requests中解决编解码的方法-1" class="headerlink" title="requests中解决编解码的方法"></a>requests中解决编解码的方法</h3><ul><li><p>response.content.decode()</p></li><li><p>response.content.decode(“gbk”)</p></li><li><p>response.text</p><h3 id="判断请求否是成功"><a href="#判断请求否是成功" class="headerlink" title="判断请求否是成功"></a>判断请求否是成功</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">assert</span> response<span class="token punctuation">.</span>status_code<span class="token operator">==</span><span class="token number">200</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="url编码"><a href="#url编码" class="headerlink" title="url编码"></a>url编码</h3></li><li><p><code>https://www.baidu.com/s?wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2</code></p><h3 id="字符串格式化的另一种方式"><a href="#字符串格式化的另一种方式" class="headerlink" title="字符串格式化的另一种方式"></a>字符串格式化的另一种方式</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token string">"传{}智播客"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="使用代理ip"><a href="#使用代理ip" class="headerlink" title="使用代理ip"></a>使用代理ip</h3></li><li><p>准备一堆的ip地址，组成ip池，随机选择一个ip来时用</p></li><li><p>如何随机选择代理ip，让使用次数较少的ip地址有更大的可能性被用到</p><ul><li>{“ip”:ip,”times”:0}</li><li>[{},{},{},{},{}],对这个ip的列表进行排序，按照使用次数进行排序</li><li>选择使用次数较少的10个ip，从中随机选择一个</li></ul></li><li><p>检查ip的可用性</p><ul><li>可以使用requests添加超时参数，判断ip地址的质量</li><li>在线代理ip质量检测的网站</li></ul></li></ul><h3 id="携带cookie请求"><a href="#携带cookie请求" class="headerlink" title="携带cookie请求"></a>携带cookie请求</h3><ul><li><p>携带一堆cookie进行请求，把cookie组成cookie池</p><h3 id="使用requests提供的session类来请求登陆之后的网站的思路"><a href="#使用requests提供的session类来请求登陆之后的网站的思路" class="headerlink" title="使用requests提供的session类来请求登陆之后的网站的思路"></a>使用requests提供的session类来请求登陆之后的网站的思路</h3></li><li><p>实例化session</p></li><li><p>先使用session发送请求，登录对网站，把cookie保存在session中</p></li><li><p>再使用session请求登陆之后才能访问的网站，session能够自动的携带登录成功时保存在其中的cookie，进行请求</p><h3 id="不发送post请求，使用cookie获取登录后的页面"><a href="#不发送post请求，使用cookie获取登录后的页面" class="headerlink" title="不发送post请求，使用cookie获取登录后的页面"></a>不发送post请求，使用cookie获取登录后的页面</h3></li><li><p>cookie过期时间很长的网站</p></li><li><p>在cookie过期之前能够拿到所有的数据，比较麻烦</p></li><li><p>配合其他程序一起使用，其他程序专门获取cookie，当前程序专门请求页面</p><h3 id="字典推导式，列表推到是"><a href="#字典推导式，列表推到是" class="headerlink" title="字典推导式，列表推到是"></a>字典推导式，列表推到是</h3><pre class="line-numbers language-python"><code class="language-python">cookies<span class="token operator">=</span><span class="token string">"anonymid=j3jxk555-nrn0wh; _r01_=1; _ga=GA1.2.1274811859.1497951251; _de=BF09EE3A28DED52E6B65F6A4705D973F1383380866D39FF5; ln_uact=mr_mao_hacker@163.com; depovince=BJ; jebecookies=54f5d0fd-9299-4bb4-801c-eefa4fd3012b|||||; JSESSIONID=abcI6TfWH4N4t_aWJnvdw; ick_login=4be198ce-1f9c-4eab-971d-48abfda70a50; p=0cbee3304bce1ede82a56e901916d0949; first_login_flag=1; ln_hurl=http://hdn.xnimg.cn/photos/hdn421/20171230/1635/main_JQzq_ae7b0000a8791986.jpg; t=79bdd322e760beae79c0b511b8c92a6b9; societyguester=79bdd322e760beae79c0b511b8c92a6b9; id=327550029; xnsid=2ac9a5d8; loginfrom=syshome; ch_id=10016; wp_fold=0"</span>cookies <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"="</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>i<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"="</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> cookies<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"; "</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">[</span>self<span class="token punctuation">.</span>url_temp<span class="token punctuation">.</span>format<span class="token punctuation">(</span>i <span class="token operator">*</span> <span class="token number">50</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="获取登录后的页面的三种方式"><a href="#获取登录后的页面的三种方式" class="headerlink" title="获取登录后的页面的三种方式"></a>获取登录后的页面的三种方式</h3></li><li><p>实例化session，使用session发送post请求，在使用他获取登陆后的页面</p></li><li><p>headers中添加cookie键，值为cookie字符串</p></li><li><p>在请求方法中添加cookies参数，接收字典形式的cookie。字典形式的cookie中的键是cookie的name对应的值，值是cookie的value对应的值</p><h3 id="requests如何获取响应的url地址和请求的url地址，如何发送post请求"><a href="#requests如何获取响应的url地址和请求的url地址，如何发送post请求" class="headerlink" title="requests如何获取响应的url地址和请求的url地址，如何发送post请求"></a>requests如何获取响应的url地址和请求的url地址，如何发送post请求</h3></li><li><p>response.url</p></li><li><p>response.request.url</p></li><li><p>requests.post(url,data={})</p><h3 id="如何发送带headers的请求和params的请求"><a href="#如何发送带headers的请求和params的请求" class="headerlink" title="如何发送带headers的请求和params的请求"></a>如何发送带headers的请求和params的请求</h3></li><li><p>requests.get(url,headers={})</p></li><li><p>requests.get(url,params = {})</p></li></ul><h3 id="如何使用代理，正向代理和反向代理的区别"><a href="#如何使用代理，正向代理和反向代理的区别" class="headerlink" title="如何使用代理，正向代理和反向代理的区别"></a>如何使用代理，正向代理和反向代理的区别</h3><ul><li><p>requests.get(url,proxies={协议:协议+ip+端口})</p></li><li><p>正向代理：客户端知道最终服务器的地址</p></li><li><p>反向代理：客户端不知道最终服务器的地址</p><h3 id="模拟登陆的三种方式"><a href="#模拟登陆的三种方式" class="headerlink" title="模拟登陆的三种方式"></a>模拟登陆的三种方式</h3></li><li><p>session</p><ul><li>实例化session（session具有的方法和requests一样）</li><li>session发送请求post请求，对方服务器设置的cookie会保存在session</li><li>session请求登录后能够访问的页面</li></ul></li><li><p>cookie放在headers中</p><ul><li>headers = {“Cookie”:”cookie字符串”}</li></ul></li><li><p>cookie转化为字典放在请求方法中</p><ul><li><p>requests.get(url,cookies={“name的值”:”values的值”})</p><h3 id="寻找登录的post地址"><a href="#寻找登录的post地址" class="headerlink" title="寻找登录的post地址"></a>寻找登录的post地址</h3></li><li><p>在form表单中寻找action对应的url地址</p><ul><li>post的数据是input标签中name的值作为键，真正的用户名密码作为值的字典，post的url地址就是action对应的url地址</li></ul></li><li><p>抓包，寻找登录的url地址</p><ul><li>勾选perserve log按钮，防止页面跳转找不到url</li><li>寻找post数据，确定参数<ul><li>参数不会变，直接用，比如密码不是动态加密的时候</li><li>参数会变<ul><li>参数在当前的响应中</li><li>通过js生成</li></ul></li></ul></li></ul></li></ul></li></ul><pre><code>### 定位想要的js- 选择会触发js时间的按钮，点击event listener，找到js的位置- 通过chrome中的search all file来搜索url中关键字- 添加断点的方式来查看js的操作，通过python来进行同样的操作### 安装第三方模块- pip install retrying- 下载源码解码，进入解压后的目录，```python setup.py install```- `***.whl` 安装方法 `pip install ***.whl`### json使用注意点- json中的字符串都是双引号引起来的  - 如果不是双引号    - eval：能实现简单的字符串和python类型的转化    - replace：把单引号替换为双引号- 往一个文件中写入多个json串，不再是一个json串，不能直接读取  - 一行写一个json串，按照行来读取  ### requests如何进行url解码，如何添加超时参数  - requests.utils.unquot(url)  - requests.utils.quot(url)  - requests.get(url,timeout=3)  ### retrying模块如何使用  ```python  @retry(stop_max_attempt_number=3)  def fun1():    pass  ```  ### 爬虫中遇到js生成的数据，怎么办  - 定位js    - search all file搜索关键字    - event listener  - 分析js，添加断点的方式  ### 寻找登录页面post的地址  - form表单的action的url地址    - input标签中用户名密码的name的值作为键，真正的用户名密码作为值的一个字典  - network 中抓包，找到post数据    - form data中      - 参数的来源        - 响应中（当前的响应或者是其他的url地址的响应）        - js生成  ### json模块如何使用，在一个文档中连续写入多个json，能够整体的读出来么  - 字符串和python类型    - json.loads(json_str)    - json.dumps(python类型,ensure_ascii=False,indent=2)  - 类文件对象中的数据和python类型的转化    - json.load(fp)[fp是类文件对象]    - json.dump(obj,fp,ensure_ascii=False,indent=2)    ### 正则使用的注意点    - `re.findall("a(.*?)b","str")`,能够返回括号中的内容,括号前后的内容起到定位和过滤的效果    - 原始字符串r，待匹配字符串中有反斜杠的时候，使用r能够忽视反斜杠带来的转义的效果    - 点号默认情况匹配不到`\n`    - `\s`能够匹配空白字符，不仅仅包含空格，还有`\t|\r\n`    ### xpath学习重点    - 使用xpath helper或者是chrome中的copy xpath都是从element中提取的数据，但是爬虫获取的是url对应的响应，往往和elements不一样    - 获取文本      - `a/text()` 获取a下的文本      - `a//text()` 获取a下的所有标签的文本      - `//a[text()='下一页']` 选择文本为下一页三个字的a标签    - `@符号`      - `a/@href`      - `//ul[@id="detail-list"]`    - `//`      - 在xpath最前面表示从当前html中任意位置开始选择      - `li//a` 表示的是li下任何一个标签    ### lxml使用注意点    - lxml能够修正HTML代码，但是可能会改错了      - 使用etree.tostring观察修改之后的html的样子，根据修改之后的html字符串写xpath    - lxml 能够接受bytes和str的字符串    - 提取页面数据的思路      - 先分组，取到一个包含分组标签的列表      - 遍历，取其中每一组进行数据的提取，不会造成数据的对应错乱      ### 正则如何进行非贪婪匹配，在div标签中的数据如何通过正则取出来      - `*?` `+?`      - `re.findall("&lt;div class="a"&gt;.*?&lt;div id=""&gt;(.*?)&lt;/div&gt;",str)` 返回列表，没有匹配到就是空列表      ### xpath如何获取文本，如何获取任意标签下的文本      - `a/text()` 只能获取a下的文本      - `a//text()` 能够获取a下的和a包含的标签的文本      - `a[text()="下一页"]`      ### xpath如何获取属性，如何对标签进行定位      - `a/@href`      - `div[@class='b']`      ### xpath中//有什么用      - `//自最前面的时候，表示html中任意位置开始选择`      - `div//*  放在节点后面的时候，能够选择当前节点下的所有的标签`      ### xpath获取某一个或者某几个      - `//a[1]` 第一个      - `//a[last()]` 最后一个      - `//a[position()&lt;=3]`      - `//a[1]|//a[3]`      ### lxml如何如何使用，如何把lxml处理之后的内容转化为字符串      - from lxml import etree      - element = etree.HTML(bytes、str) #把字符串转化为element对象      - etree.tostring(element) #把element对象转化为字符串      - element.xpath("xpath_str")      ### xpath的包含      - `//div[contains(@class,'i')]`      ### 实现爬虫的套路      - 准备url        - 准备start_url          - url地址规律不明显，总数不确定          - 通过代码提取下一页的url            - xpath            - 寻找url地址，部分参数在当前的响应中（比如，当前页码数和总的页码数在当前的响应中）        - 准备url_list          - 页码总数明确          - url地址规律明显      - 发送请求，获取响应        - 添加随机的User-Agent,反反爬虫        - 添加随机的代理ip，反反爬虫        - 在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie        - cookie的处理可以使用session来解决        - 准备一堆能用的cookie，组成cookie池          - 如果不登录            - 准备刚开始能够成功请求对方网站的cookie，即接收对方网站设置在response的cookie            - 下一次请求的时候，使用之前的列表中的cookie来请求          - 如果登录            - 准备多个账号            - 使用程序获取每个账号的cookie            - 之后请求登录之后才能访问的网站随机的选择cookie      - 提取数据        - 确定数据的位置          - 如果数据在当前的url地址中            - 提取的是列表页的数据              - 直接请求列表页的url地址，不用进入详情页            - 提取的是详情页的数据              - 1. 确定url              - 2. 发送请求              - 3. 提取数据              - 4. 返回          - 如果数据不在当前的url地址中            - 在其他的响应中，寻找数据的位置              - 1. 从network中从上往下找              - 2. 使用chrome中的过滤条件，选择出了js,css,img之外的按钮              - 3. 使用chrome的search all file，搜索数字和英文        - 数据的提取          - xpath,从html中提取整块的数据，先分组，之后每一组再提取          - re，提取max_time,price,html中的json字符串          - json      - 保存        - 保存在本地，text,json,csv        - 保存在数据库        ### lxml模块如何使用        - from lxml import etree        - element = etree.HMTL(str,bytes)        - element.xpath("xpath")        - etree.tostring(element) #把element转化为字符串        ### xpath有哪些常用方法        - `//` 从任意位置选择节点          - `//a/text()` a下的文本          - `//a//text()` a下所有的文本        - `.` 当前路径        - `@符号`          - `a/@href`          - `div[@class='a']`        - `text()`          - `a[text()='下一页']`        - `..` 上一级        - `//a[1]`        - `//a[last()]`        - `//a[postion()&lt;4]`        - `//a[1]|//a[5]`        - `a[contains(text(),"下一页")]`        ### queue模块如何使用        - from queue import Queue        - 实例化        - queue.put() #get计数减一        - queue.get()        - queue.task_doen() #计数减一        - queue.join() #让主线程阻塞        #### 验证码的识别        - url不变，验证码不变          - 请求验证码的地址，获得相应，识别        - url不变，验证码会变          - 思路：对方服务器返回验证码的时候，会和每个用户的信息和验证码进行一个对应，之后，在用户发送post请求的时候，会对比post请求中法的验证码和当前用户真正的存储在服务器端的验证码是否相同          - 1.实例化session          - 2.使用seesion请求登录页面，获取验证码的地址          - 3.使用session请求验证码，识别          - 4.使用session发送post请求’        - 使用selenium登录，遇到验证码          - url不变，验证码不变，同上          - url不变，验证码会变            - 1.selenium请求登录页面，同时拿到验证码的地址            - 2.获取登录页面中driver中的cookie，交给requests模块发送验证码的请求，识别            - 3.输入验证码，点击登录        ### selenium使用的注意点        - 获取文本和获取属性          - 先定位到元素，然后调用`.text`或者`get_attribute`方法来去        - selenium获取的页面数据是浏览器中elements的内容        - find_element和find_elements的区别          - find_element返回一个element，如果没有会报错          - find_elements返回一个列表，没有就是空列表          - 在判断是否有下一页的时候，使用find_elements来根据结果的列表长度来判断        - 如果页面中含有iframe、frame，需要先调用driver.switch_to.frame的方法切换到frame中才能定位元素        - selenium请求第一页的时候回等待页面加载完了之后在获取数据，但是在点击翻页之后，hi直接获取数据，此时可能会报错，因为数据还没有加载出来，需要time.sleep(3)        - selenium中find_element_by_class_name智能接收一个class对应的一个值，不能传入多个        ```        db.stu.aggregate({$group:{_id:"$name",counter:{$sum:2}}})        db.stu.aggregate({$group:{_id:null,counter:{$sum:1}}})        db.stu.aggregate({$group:{_id:"$gender",name:{$push:"$name"}}})        db.stu.aggregate({$group:{_id:"$gender",name:{$push:"$$ROOT"}}})        db.tv3.aggregate(          {$group:{_id:{"country":"$country",province:"$province",userid:"$userid"}}},          {$group:{_id:{country:"$_id.country",province:"$_id.province"},count:{$sum:1}}},          {$project:{country:"$_id.country",province:"$_id.province",count:"$count",_id:0}}          )        db.stu.aggregate(          {$match:{age:{$gt:20}}},          {$group:{_id:"$gender",count:{$sum:1}}}          )        db.t2.aggregate(          {$unwind:"$size"}          )        db.t3.aggregate(          {$unwind:"$tags"},          {$group:{_id:null,count:{$sum:1}}}          )        db.t3.aggregate(          {$unwind:{path:"$size",preserveNullAndEmptyArrays:true}}          )        ```        ### selenium 如何使用        - from selenium import webdriver        - driver = webdriver.Chrome()        - driver.get(url)        - driver.quit()        ### selenium如何定位，如何获取属性和文本        - driver.find_element  返回一个对象，没有会报错        - driver.find_elements 返回一个列表，空列表        - driver.find_element_by_id()        - driver.find_element_by_class_name()        - driver.find_element_by_xpath()        - driver.find_element_by_link_text()        - driver.find_element_by_id().text        - driver.find_element_by_id().get_attribute()        ### selenium如何切换iframe中        - driver.switch_to.frame(frame的id,name,driver.find_element_by_xpath("//a[1]"))        ### 遇到验证码如何处理        - url地址对应的验证码会变          - 1.实例化一个session          - 2.session请求登录页面，获取验证码的地址          - 3.session请求验证码，识别          - 4.session发送post请求        - url地址对应的验证码不会变          - 请求验证码地址，识别          ### mongodb插入数据          - db.collecion.insert({}) 插入数据，`_id`存在就报错          - db.collection.save({}) 插入数据，`_id`存在会更新          ### mongodb的更新操作          - `db.test1000.update({name:"xiaowang"},{name:"xiaozhao"})`          - 把name为xiaowang的数据替换为`{name:"xiaozhao"}`          - `db.test1000.update({name:"xiaohong"},{$set:{name:"xiaozhang"}})`          - 把name为xiaowang的数据name的值更新为xiaozhang          - `db.test1000.update({name:"xiaozhang"},{$set:{name:"xiaohong"}},{multi:true})`          - `{multi:true}`达到更新多条的目的          ### mongodb删除          - `db.test1000.remove({name:"xiaohong"},{justOne:true})`          - 默认情况会删除所有满足条件的数据，`{justOne:true}`能达到只删除一条的效果          ### mongodb的count方法          - `db.collection.find({条件}).count()`          - `db.collection.count({})`          ### mongodb的投影          - 投影:选择返回结果的字段          - `db.collection.find({条件},{name:1,_id:0})`            - 1.`_id`默认会显示，置为0不显示            - 2.出了`_id`之外的其他字段，如果不显示，不写，不能写为0          ### $group的注意点          - `$group`对应的字典中有几个键，结果中就有几个键          - 分组依据需要放到`_id`后面          - 取不同的字段的值需要使用$,`$gender`,`$age`          - 取字典嵌套的字典中的值的时候`$_id.country`          - 能够同时按照多个键进行分组`{$group:{_id:{country:"$country",province:"$province"}}}`            - 结果是：`{_id:{country:"",province:""}`          ### 编辑器写mongodb语句          ```          db.stu.find(            {$or:[{age:{$gte:20}},{hometown:{$in:["桃花岛","华⼭"]}}]}            )          #按照gender进行分组，获取不同组数据的个数和平均年龄          db.stu.aggregate(            {$group:{_id:"$gender",count:{$sum:1},avg_age:{$avg:"$age"}}},            {$project:{gender:"$_id",count:1,avg_age:"$avg_age",_id:0}}            )          # 按照hometown进行分组，获取不同组的平均年龄          db.stu.aggregate(            {$group:{_id:"$hometown",mean_age:{$avg:"$age"}}}            )          #使用$group统计整个文档          db.stu.aggregate(            {$group:{_id:null,count:{$sum:1},mean_age:{$avg:"$age"}}}            )          #选择年龄大于20的学生，观察男性和女性有多少人          db.stu.aggregate(            {$match:{$or:[{age:{$gt:20}},{hometown:{$in:["蒙古","⼤理"]}}]}},            {$group:{_id:"$gender",count:{$sum:1}}},            {$project:{_id:0,gender:"$_id",count:1}}            )          #page37页练习          db.tv3.aggregate(            {$group:{_id:{country:"$country",province:"$province",userid:"$userid"}}},            {$group:{_id:{country:"$_id.country",province:"$_id.province"},count:{$sum:1}}},            {$project:{country:"$_id.country",province:"$_id.province",count:1,_id:0}}            )          ```          ### mongodb的增删改查如何操作          - 增            - db.collection.insert() #`_id`相同会报错            - db.collection.save() #`_id`相同会更新其余的字段          - 删            - db.collection.remove({},{justOne:ture})          - 更新            - `db.collection.update({},{$set:{name:"a"}},{multi:true})`          - 查询            - db.collection.find({})            - db.collection.find({}).pretty()          ### mongodb统计数量，mongodb的投影操作          - db.collection.count({})          - db.collection.find({}).count          - 投影：设置返回的字段          - db.collection.find({},{_id:0,name:1})          ### mongodb的比较运算符(大于，小于等于等)，范围运算符（in，not in），逻辑运算符（and ，or）          - 大于，大于等于 `$gt,$get`          - `$lt,$lte 小于，小于等于`          - $ne 不等于          - 在 `$in`            - `db.collection.find({name:{$in:["a","b","c"]}})`          - 不在 `$nin`            - `db.collection.find({name:{$nin:["a","b","c"]}})`          - 和            - `db.collection.find({name:"a",age:20})`          - or            - `bd.collection.find({$or:[{name:"a"},{age:20}]})`          ### mongodb的排序和消除重复          - 排序            - db.collection.find({}).sort({age:1})          - distinct            - db.collection.distinct("gender",{age:{$gt:18}})            - 返回数组          ### 聚合操作的分组和计数如何使用，如何修改输出数据的样式，          - 分组            - `db.collection.aggregate({$group:{_id:"$age",count:{$sum:1}}})`          - `$project`            - `db.collection.aggregate(              {$group:{_id:"$age",count:{$sum:1}}},              {$project:{_id:0,age:"$_id",count:1}}              )`          ### 聚合操作如何匹配内容          - `$match`            - `db.collection.aggregate(              {$match:{gender:true},              {$group:{_id:"$age",count:{$sum:1}}},              {$project:{_id:0,age:"$_id",count:1}}              )`              ### mongodb mysql redis的区别和使用场景              - mysql是关系型数据库，支持事物              - mongodb，redis非关系型数据库，不支持事物              - mysql，mongodb，redis的使用根据如何方便进行选择                - 希望速度快的时候，选择mongodb或者是redis                - 数据量过大的时候，选择频繁使用的数据存入redis，其他的存入mongodb                - mongodb不用提前建表建数据库，使用方便，字段数量不确定的时候使用mongodb                - 后续需要用到数据之间的关系，此时考虑mysql              ### 爬虫数据去重，实现增量式爬虫              - 使用数据库建立关键字段（一个或者多个）建立索引进行去重              - 根据url地址进行去重                - 使用场景：                  - url地址对应的数据不会变的情况，url地址能够唯一判别一个条数据的情况                - 思路                  - url存在redis中                  - 拿到url地址，判断url在redis的url的集合中是够存在                  - 存在：说明url已经被请求过，不再请求                  - 不存在：url地址没有被请求过，请求，把该url存入redis的集合中                - 布隆过滤器                  - 使用多个加密算法加密url地址，得到多个值                  - 往对应值的位置把结果设置为1                  - 新来一个url地址，一样通过加密算法生成多个值                  - 如果对应位置的值全为1，说明这个url地址已经抓过                  - 否则没有抓过，就把对应位置的值设置为1              - 根据数据本省进行去重                - 选择特定的字段，使用加密算法（md5，sha1）讲字段进行假面，生成字符串，存入redis的集合中                - 后续新来一条数据，同样的方法进行加密，如果得到的字符串在redis中存在，说明数据存在，对数据进行更新，否则说明数据不存在，直接插入              ### page50练习              ```              db.tv1.aggregate(                {$project:{title:1,_id:0,count:"$rating.count",rate:"$rating.value",country:"$tv_category"}},                {$match:{rate:{$gt:8}}},                {$group:{_id:"$country",count:{$sum:1}}},                {$project:{_id:0,country:"$_id",count:1}}                )              ```              ### mongodb的索引如何使用              - `db.collection.ensureIndex({name:1})`              - `db.colelction.getIndexes()`              - `db.collection.dropIdex({name:1})`              - 创建唯一索引 `db.collection.ensureIndex({},{unique:ture})`              - 创建复合索引 `db.collection.ensureIndex({name：1，age:1},{unique:ture})`              ### pymongo如何使用              - from pymongo import MongoClient              - client = MongoClient(host,port)  #建立连接              - colelction = client["db"]["collection"]              - 增                - collection.insert({})                - collection.insert_many([{},{}])              - 删除                - collection.delete_one({条件})                - collection.delete_many()              - 更新                - colelction.update_one                - collection.update_many              - 查询                - collection.find({})                  - 返回cursor，能够迭代，只能迭代一些                - collection.find_one()              ### scrapy的数据流程              - 调取器---》request对象---》引擎---》下载中间件---》下载器              - 下载器发送请求，获取响应---》response---》下载中间件---》引擎---》爬虫中间件---》spider              - spider提取数据--- 》引擎---》pipeline              - spider提取的url地址---》构造request对象---》爬虫中渐渐---》引擎---》调度器              ### scrapy的使用流程              - 创建项目 scrapy startproject 项目名              - 创建爬虫 scrapy genspider spider_name allow_domain              - 完善爬虫                - start_url,response --&gt; parse                - 数据yield 通过传递给管道                - 能够yield 的数据类型，dict，request，Item，None              - 管道                - 开启管道                  - 开启管道，键:位置，值：距离引擎的远近，越小越近，说句越先经过                  ### logging 模块的使用                  - scrapy                    - settings中设置LOG_LEVEL=“WARNING”                    - settings中设置LOG_FILE="./a.log"  #设置日志保存的位置，设置会后终端不会显示日志内容                    - import logging,实例化logger的方式在任何文件中使用logger输出内容                  - 普通项目中                    - import logging                    - logging.basicConfig(...) #设置日志输出的样式，格式                    - 实例化一个`logger=logging.getLogger(__name__)`                    - 在任何py文件中调用logger即可                    ### scrapy数据流程                    - 调度器---》request---》引擎---》下载中间件---》下载器                    - 下载器发送请求，获取resposne，---》response---&gt;下载中间件---》引擎---》爬虫中间件---》spider                    - spider 提取数据---》引擎---》pipeline                    - spider 提取url地址--》构造request---》爬虫中间件---》引擎---》调度器                    ### scrapy如何发送请求，能够携带什么参数                    - scrapy.Request(url,callback,meta,dont_filter)                    - dont_filter=True 表示请求过的url地址还会继续被请求                    ### scrapy如何把数据从一个解析函数传递到另一个，为什么需要这样做                    - meta是个字典类型，meta = {"item":item}                    - response.meta["item"]                    ### scrapy中Item是什么，如何使用                    - Item 类，继承自scarpy.Item,name=scrapy.Field()                    - Item 定义那些字段我们需要抓取                    - 使用和字典一样                    - 在mongodb中插入数据的时候 dict(item)                    ### pipeline中open_spider和close_spider是什么                    - open_spdier 爬虫开启执行一次，只有一次                    - close_spider 爬虫关闭的时候执行一次，只有一次                    ### crawlspider的使用                    - 常见爬虫 scrapy genspider -t crawl 爬虫名 allow_domain                    - 指定start_url，对应的响应会进过rules提取url地址                    - 完善rules，添加Rule ` Rule(LinkExtractor(allow=r'/web/site0/tab5240/info\d+\.htm'), callback='parse_item'),`                    - 注意点:                      - url地址不完整，crawlspider会自动补充完整之后在请求                      - parse函数不能定义，他有特殊的功能需要实现                      - callback：连接提取器提取出来的url地址对应的响应交给他处理                      - follow：连接提取器提取出来的url地址对应的响应是否继续被rules来过滤                      ### crawlspider如何使用                      - 创建爬虫 scarpy genspider -t crawl spider_name allow_domain                      - 完善spider                        - 1.start_url                        - 2.完善rules                          - 元组                          - Rule(LinkExtractor,callback，follow)                            - LinkExtractor 连接提取器，提取url                            - callback url的响应会交给该callback处理                            - follow= True url的响应会继续被Rule提取地址                        - 3.完善callback                      ### 下载中间件如何使用                      - 定义类                      - process_request 处理请求，不需要return                      - process_response  处理响应，需要return request response                      - settings中开启                      ### scrapy如何模拟登陆                      - 携带cookie登录                        - 准备cookie字典                        - scrapy.Request(url,callback,cookies=cookies_dict)                      - scrapy.FormRequest(post_url,formdata={},callback)                      - scrapy.FormRequest.from_response(response,formdata,callback)                      ### request对象什么时候入队                      - dont_filter = True ,构造请求的时候，把dont_filter置为True，该url会被反复抓取（url地址对应的内容会更新的情况）                      - 一个全新的url地址被抓到的时候，构造request请求                      - url地址在start_urls中的时候，会入队，不管之前是否请求过                        - 构造start_url地址的请求时候，dont_filter = True                      ```python                      def enqueue_request(self, request):                          if not request.dont_filter and self.df.request_seen(request):                              # dont_filter=False Ture  True request指纹已经存在  #不会入队                              # dont_filter=False Ture  False  request指纹已经存在 全新的url  #会入队                              # dont_filter=Ture False  #会入队                              self.df.log(request, self.spider)                              return False                          self.queue.push(request) #入队                          return True                      ```                      ### scrapy_redis去重方法                      - 使用sha1加密request得到指纹                      - 把指纹存在redis的集合中                      - 下一次新来一个request，同样的方式生成指纹，判断指纹是否存在reids的集合中                      ### 生成指纹                      ```python                      fp = hashlib.sha1()                      fp.update(to_bytes(request.method))  #请求方法                      fp.update(to_bytes(canonicalize_url(request.url))) #url                      fp.update(request.body or b'')  #请求体                      return fp.hexdigest()                      ```                      ### 判断数据是否存在redis的集合中，不存在插入                      ```python                      added = self.server.sadd(self.key, fp)                      return added != 0                      ```                      ### domz的案例给我们展示了一个什么样的爬虫，如何实现的                      - 增量式爬虫（爬取过的生成指纹），断点续爬（带爬取的request存在redis中）                      - settings中设置                        - 指定的去重的类                        - 指定调度器的类                        - redis_url                        - redisPipeline                      ### request对象什么时候入队                      - dont_filter=True                      - 全新的url                      - url在start_urls中的                        - start_request                      ### scrapy_redis去重方法                      - 使用sha1得到requests的指纹                      - 指纹存在redis的集合中                      - 下一次request，生成指纹，判断是否存在redis的集合中                      ### 生成指纹                      - fp = hashlib.sha1()                      - fp.update(request.method)                      - fp.update(request.body or b"")                      - fp.update(url)                      - fp.hexdigest()                      ### 判断数据是否存在redis的集合中，不存在插入                      - added = server.sadd(key,fp)                      - return added == 0 #Ture 插入失败，已经存在                      ### 爬虫项目                      - 项目名字                        - request+selenium爬虫                      - 项目周期                      - 项目介绍                        - 爬了XXXXX，XXX，XXX，等网站，获取网站上的XXX，XXX，XXX，数据，每个月定时抓取XXX数据，使用该数据实现了XXX，XXX，XX，                      - 开发环境                        - linux+pycharm+requests+mongodb+redis+crontab+scrapy_redis+ scarpy + mysql+gevent+celery+threading                      - 使用技术                        - 使用requests...把数据存储在mongodb中                        - 使用crontab实现程序的定时启动抓取                        - url地址的去重                          - 使用redis的集合，把request对象的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现基于url地址的增量式爬虫                          - 布隆过滤                        - 对数据的去重                          - 把数据的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现增量式爬虫                        - 反扒                          - 代理ip                            - 购买了第三的代理ip，组成代理ip池，其中的ip没两天更新一次，同时使用单独的程序来检查代理ip的可用                          - cookie                            - 准备了XX个账号，使用requests获取账号的对应的cookie，存储在redis中，后续发送请求的时候随机选择cookie                            - 使用selenium来进行模拟登陆，获取cookie，保存在Redis中                          - 数据通过js生成                            - 分析js，通过chrome浏览器定位js的位置，寻找js生成数据的方式                            - 通过selenium来模拟页面的加载内容，获取页面动态加载后的数据                        - 提高爬虫效率                          - 使用多线，线程池，协程，celery来完成爬虫                          - 使用scrapy框架来实现爬虫，                            - 不能断点续爬，请求过的url地址不能持久化                              - 使用scrapy_redis                            - 不能对数据进行去重                              - 把数据的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现增量式爬虫                          - scrapy_redis                            - domz实现增量式，持久化的爬虫                            - 实现分布式爬虫                      - 项目名字                        - scarpy爬虫                      - 项目周期                      - 项目介绍                        - 爬了XXXXX，XXX，XXX，等网站，获取网站上的XXX，XXX，XXX，数据，每个月定时抓取XXX数据，使用该数据实现了XXX，XXX，XX，                      - 开发环境                        - linux+pycharm+requests+mongodb+redis+crontab+scrapy_redis+ scarpy + mysql+gevent+celery+threading                      - 使用技术                        - 使用requests...把数据存储在mongodb中                        - 使用crontab实现程序的定时启动抓取                        - url地址的去重                          - 使用redis的集合，把request对象的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现基于url地址的增量式爬虫                          - 布隆过滤                        - 对数据的去重                          - 把数据的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现增量式爬虫                        - 反扒                          - 代理ip                            - 购买了第三的代理ip，组成代理ip池，其中的ip没两天更新一次，同时使用单独的程序来检查代理ip的可用                          - cookie                            - 准备了XX个账号，使用requests获取账号的对应的cookie，存储在redis中，后续发送请求的时候随机选择cookie                            - 使用selenium来进行模拟登陆，获取cookie，保存在Redis中                          - 数据通过js生成                            - 分析js，通过chrome浏览器定位js的位置，寻找js生成数据的方式                            - 通过selenium来模拟页面的加载内容，获取页面动态加载后的数据                        - 提高爬虫效率                          - 使用多线，线程池，协程，celery来完成爬虫                          - 使用scrapy框架来实现爬虫，                            - 不能断点续爬，请求过的url地址不能持久化                              - 使用scrapy_redis                            - 不能对数据进行去重                              - 把数据的XXX字段通过sha1生成指纹，放入redis的集合中进行去重，实现增量式爬虫                          - scrapy_redis                            - domz实现增量式，持久化的爬虫                            - 实现分布式爬虫</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
